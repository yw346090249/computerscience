<!DOCTYPE html>
<html>
<head>
<title>Azure Stream Analytics HOL (Node)</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
<style type="text/css">
.highlight  { background: #ffffff; }
.highlight .c { color: #999988; font-style: italic } /* Comment */
.highlight .err { color: #a61717; background-color: #e3d2d2 } /* Error */
.highlight .k { font-weight: bold } /* Keyword */
.highlight .o { font-weight: bold } /* Operator */
.highlight .cm { color: #999988; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #999999; font-weight: bold } /* Comment.Preproc */
.highlight .c1 { color: #999988; font-style: italic } /* Comment.Single */
.highlight .cs { color: #999999; font-weight: bold; font-style: italic } /* Comment.Special */
.highlight .gd { color: #000000; background-color: #ffdddd } /* Generic.Deleted */
.highlight .gd .x { color: #000000; background-color: #ffaaaa } /* Generic.Deleted.Specific */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #aa0000 } /* Generic.Error */
.highlight .gh { color: #999999 } /* Generic.Heading */
.highlight .gi { color: #000000; background-color: #ddffdd } /* Generic.Inserted */
.highlight .gi .x { color: #000000; background-color: #aaffaa } /* Generic.Inserted.Specific */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #555555 } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #aaaaaa } /* Generic.Subheading */
.highlight .gt { color: #aa0000 } /* Generic.Traceback */
.highlight .kc { font-weight: bold } /* Keyword.Constant */
.highlight .kd { font-weight: bold } /* Keyword.Declaration */
.highlight .kp { font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #445588; font-weight: bold } /* Keyword.Type */
.highlight .m { color: #009999 } /* Literal.Number */
.highlight .s { color: #d14 } /* Literal.String */
.highlight .na { color: #008080 } /* Name.Attribute */
.highlight .nb { color: #0086B3 } /* Name.Builtin */
.highlight .nc { color: #445588; font-weight: bold } /* Name.Class */
.highlight .no { color: #008080 } /* Name.Constant */
.highlight .ni { color: #800080 } /* Name.Entity */
.highlight .ne { color: #990000; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #990000; font-weight: bold } /* Name.Function */
.highlight .nn { color: #555555 } /* Name.Namespace */
.highlight .nt { color: #000080 } /* Name.Tag */
.highlight .nv { color: #008080 } /* Name.Variable */
.highlight .ow { font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #009999 } /* Literal.Number.Float */
.highlight .mh { color: #009999 } /* Literal.Number.Hex */
.highlight .mi { color: #009999 } /* Literal.Number.Integer */
.highlight .mo { color: #009999 } /* Literal.Number.Oct */
.highlight .sb { color: #d14 } /* Literal.String.Backtick */
.highlight .sc { color: #d14 } /* Literal.String.Char */
.highlight .sd { color: #d14 } /* Literal.String.Doc */
.highlight .s2 { color: #d14 } /* Literal.String.Double */
.highlight .se { color: #d14 } /* Literal.String.Escape */
.highlight .sh { color: #d14 } /* Literal.String.Heredoc */
.highlight .si { color: #d14 } /* Literal.String.Interpol */
.highlight .sx { color: #d14 } /* Literal.String.Other */
.highlight .sr { color: #009926 } /* Literal.String.Regex */
.highlight .s1 { color: #d14 } /* Literal.String.Single */
.highlight .ss { color: #990073 } /* Literal.String.Symbol */
.highlight .bp { color: #999999 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #008080 } /* Name.Variable.Class */
.highlight .vg { color: #008080 } /* Name.Variable.Global */
.highlight .vi { color: #008080 } /* Name.Variable.Instance */
.highlight .il { color: #009999 } /* Literal.Number.Integer.Long */
.pl-c {
    color: #969896;
}

.pl-c1,.pl-mdh,.pl-mm,.pl-mp,.pl-mr,.pl-s1 .pl-v,.pl-s3,.pl-sc,.pl-sv {
    color: #0086b3;
}

.pl-e,.pl-en {
    color: #795da3;
}

.pl-s1 .pl-s2,.pl-smi,.pl-smp,.pl-stj,.pl-vo,.pl-vpf {
    color: #333;
}

.pl-ent {
    color: #63a35c;
}

.pl-k,.pl-s,.pl-st {
    color: #a71d5d;
}

.pl-pds,.pl-s1,.pl-s1 .pl-pse .pl-s2,.pl-sr,.pl-sr .pl-cce,.pl-sr .pl-sra,.pl-sr .pl-sre,.pl-src,.pl-v {
    color: #df5000;
}

.pl-id {
    color: #b52a1d;
}

.pl-ii {
    background-color: #b52a1d;
    color: #f8f8f8;
}

.pl-sr .pl-cce {
    color: #63a35c;
    font-weight: bold;
}

.pl-ml {
    color: #693a17;
}

.pl-mh,.pl-mh .pl-en,.pl-ms {
    color: #1d3e81;
    font-weight: bold;
}

.pl-mq {
    color: #008080;
}

.pl-mi {
    color: #333;
    font-style: italic;
}

.pl-mb {
    color: #333;
    font-weight: bold;
}

.pl-md,.pl-mdhf {
    background-color: #ffecec;
    color: #bd2c00;
}

.pl-mdht,.pl-mi1 {
    background-color: #eaffea;
    color: #55a532;
}

.pl-mdr {
    color: #795da3;
    font-weight: bold;
}

.pl-mo {
    color: #1d3e81;
}
.task-list {
padding-left:10px;
margin-bottom:0;
}

.task-list li {
    margin-left: 20px;
}

.task-list-item {
list-style-type:none;
padding-left:10px;
}

.task-list-item label {
font-weight:400;
}

.task-list-item.enabled label {
cursor:pointer;
}

.task-list-item+.task-list-item {
margin-top:3px;
}

.task-list-item-checkbox {
display:inline-block;
margin-left:-20px;
margin-right:3px;
vertical-align:1px;
}
</style>
</head>
<body>
<p><a name="HOLTitle"></a></p>
<h1>Analyzing Data in Real Time with Azure Stream Analytics</h1>
<hr>
<p><a name="Overview"></a></p>
<h2>Overview</h2>
<p>Azure Stream Analytics is a cloud-based service for ingesting high-velocity data streaming from devices, sensors, applications, Web sites, and other data sources and analyzing that data in real time. It supports a SQL-like query language that works over dynamic data streams and makes analyzing constantly changing data no more difficult than performing queries on static data stored in traditional databases. With Azure Stream Analytics, you can set up jobs that analyze incoming data for anomalies or information of interest and record the results, present notifications on dashboards, or even fire off alerts to mobile devices. And all of it can be done at low cost and with a minimum of effort.</p>
<p>Scenarios for the application of real-time data analytics are legion and include fraud detection, identity-theft protection, optimizing the allocation of resources (think of an Uber-like transportation service that sends drivers to areas of increasing demand <em>before</em> that demand peaks), click-stream analysis on Web sites, shopping suggestions on retail-sales sites, and countless others. Having the ability to process data <em>as it comes in</em> rather than waiting until after it has been aggregated offers a competitive advantage to businesses that are agile enough to make adjustments on the fly.</p>
<p>In this lab, you'll create an Azure Stream Analytics job and use it to analyze data streaming in from simulated Internet of Things (IoT) devices. And you'll see how simple it is to monitor real-time data streams for information of significance to your research or business.</p>
<p><a name="Objectives"></a></p>
<h3>Objectives</h3>
<p>In this hands-on lab, you will learn how to:</p>
<ul>
<li>Create an Azure event hub and use it as a Stream Analytics input</li>
<li>Create a Stream Analytics job and test queries on sample data streams</li>
<li>Run a Stream Analytics job and perform queries on live data streams</li>
<li>Store Stream Analytics output in Azure storage blobs</li>
</ul>
<p><a name="Prerequisites"></a></p>
<h3>Prerequisites</h3>
<p>The following is required to complete this hands-on lab:</p>
<ul>
<li>An active Microsoft Azure subscription. If you don't have one, <a href="http://aka.ms/WATK-FreeTrial">sign up for a free trial</a>.</li>
<li><a href="https://nodejs.org">Node.js</a></li>
</ul>
<hr>
<p><a name="Exercises"></a></p>
<h2>Exercises</h2>
<p>This hands-on lab includes the following exercises:</p>
<ul>
<li><a href="#Exercise1">Exercise 1: Create an event hub</a></li>
<li><a href="#Exercise2">Exercise 2: Create a shared-access signature token</a></li>
<li><a href="#Exercise3">Exercise 3: Send events to the event hub</a></li>
<li><a href="#Exercise4">Exercise 4: Create a Stream Analytics job</a></li>
<li><a href="#Exercise5">Exercise 5: Prepare queries and test with sample data</a></li>
<li><a href="#Exercise6">Exercise 6: Analyze a live data stream</a></li>
</ul>
<p>Estimated time to complete this lab: <strong>60</strong> minutes.</p>
<p><a name="Exercise1"></a></p>
<h2>Exercise 1: Create an event hub</h2>
<p>Azure Stream Analytics supports several types of input, including input from Azure blobs and input from Azure event hubs. Of the two, the latter is typically more interesting because in the IoT world, data is easily transmitted to Azure event hubs through field gateways (for devices that are not IP-capable) or cloud gateways (for devices that <em>are</em> IP-capable), and a single Azure event hub can handle millions of events per second transmitted from devices spread throughout the world.</p>
<p>In this exercise, you'll create an Azure event hub to provide input to Azure Stream Analytics and configure it to so that it can be accessed safely and securely by IoT devices and gateways.</p>
<ol>
<li>
<p>In your browser, navigate to the <a href="https://portal.azure.com">Azure Portal</a>. If you are asked to sign in, do so using your Microsoft account.</p>
</li>
<li>
<p>In the portal, click <strong>+ New</strong>, followed by <strong>Internet of Things</strong> and <strong>Event Hubs</strong>.</p>
<p><a href="Images/new-event-hub.png" target="_blank"><img src="Images/new-event-hub.png" alt="Adding a new event hub" style="max-width:100%;"></a></p>
<p><em>Adding a new event hub</em></p>
</li>
<li>
<p>Type a namespace name into the <strong>Name</strong> box. The name must be unique within Azure, so you will probably have to use something other than the name in the screen shot below. (A green check mark will appear in the box when the name you've entered is one that Azure will accept.) Select <strong>Create new</strong> under <strong>Resource group</strong> and enter the resource-group name "StreamAnalyticsResourceGroup" (without quotation marks). Choose the region closest to you in the <strong>Location</strong> drop-down, and then click the <strong>Create</strong> button.</p>
<p><a href="Images/create-namespace.png" target="_blank"><img src="Images/create-namespace.png" alt="Creating a namespace" style="max-width:100%;"></a></p>
<p><em>Creating a namespace</em></p>
</li>
<li>
<p>Click <strong>Resource groups</strong> in the ribbon on the left, and then click the "StreamAnalyticsResourceGroup" resource group created in the previous step.</p>
<p><a href="Images/open-resource-group.png" target="_blank"><img src="Images/open-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Wait until "Deploying" changes to "Succeeded." Then click the namespace whose name you specified in Step 3.</p>
<blockquote>
<p>Click the browser's <strong>Refresh</strong> button occasionally to update the deployment status. Clicking the <strong>Refresh</strong> button in the resource-group blade refreshes the list of resources in the resource group, but does not reliably update the deployment status.</p>
</blockquote>
<p><a href="Images/open-namespace.png" target="_blank"><img src="Images/open-namespace.png" alt="Opening the namespace" style="max-width:100%;"></a></p>
<p><em>Opening the namespace</em></p>
</li>
<li>
<p>Click <strong>+ Event Hub</strong> to add an event hub to the namespace.</p>
<p><a href="Images/add-event-hub.png" target="_blank"><img src="Images/add-event-hub.png" alt="Adding an event hub" style="max-width:100%;"></a></p>
<p><em>Adding an event hub</em></p>
</li>
<li>
<p>Type "inputhub" (without quotation marks) into the <strong>Name</strong> box. Then click the <strong>Create</strong> button.</p>
<p><a href="Images/create-event-hub.png" target="_blank"><img src="Images/create-event-hub.png" alt="Creating an event hub" style="max-width:100%;"></a></p>
<p><em>Creating an event hub</em></p>
</li>
<li>
<p>Wait a moment for the event hub to be created. Then scroll to the bottom of the blade and click the event hub name.</p>
<p><a href="Images/open-event-hub.png" target="_blank"><img src="Images/open-event-hub.png" alt="Opening the event hub" style="max-width:100%;"></a></p>
<p><em>Opening the event hub</em></p>
</li>
<li>
<p>In order to transmit events to the event hub from an application or device, you need to create a shared-access policy that includes Send permission. To begin, click <strong>Shared access policies</strong>, and then click <strong>+ Add</strong>.</p>
<p><a href="Images/new-shared-access-policy.png" target="_blank"><img src="Images/new-shared-access-policy.png" alt="Adding a shared-access policy" style="max-width:100%;"></a></p>
<p><em>Adding a shared-access policy</em></p>
</li>
<li>
<p>Type "SendPolicy" (without quotation marks) into the <strong>Policy name</strong> box and check the <strong>Send</strong> box. Then click the <strong>Create</strong> button to create the new policy.</p>
<p><a href="Images/create-send-policy.png" target="_blank"><img src="Images/create-send-policy.png" alt="Creating a send policy" style="max-width:100%;"></a></p>
<p><em>Creating a send policy</em></p>
</li>
<li>
<p>Wait a moment for <strong>SendPolicy</strong> to appear in the policies list, and then click it.</p>
<p><a href="Images/open-send-policy.png" target="_blank"><img src="Images/open-send-policy.png" alt="Opening the policy" style="max-width:100%;"></a></p>
<p><em>Opening the policy</em></p>
</li>
<li>
<p>Click the <strong>Copy</strong> button to the right of the <strong>PRIMARY KEY</strong> box to copy the policy's shared-access key to the clipboard. Then temporarily save the key by pasting it into your favorite text editor. You'll need this key in the next exercise.</p>
<p><a href="Images/copy-access-key.png" target="_blank"><img src="Images/copy-access-key.png" alt="Copying the primary key to the clipboard" style="max-width:100%;"></a></p>
<p><em>Copying the primary key to the clipboard</em></p>
</li>
</ol>
<p>You have created an event hub that can ingest events and be used as the source of input to a Stream Analytics job. You have also created a policy that allows holders of that policy to send events to the event hub. The next step is to generate a security token that can be used to authenticate calls to the event hub.</p>
<p><a name="Exercise2"></a></p>
<h2>Exercise 2: Create a shared-access signature token</h2>
<p>Applications, devices, or gateways can send events to event hubs using the <a href="https://msdn.microsoft.com/en-us/library/azure/Dn790674.aspx">Azure Event Hubs REST API</a>. Each request transmitted via this API must include a valid <a href="https://azure.microsoft.com/en-us/documentation/articles/service-bus-shared-access-signature-authentication/">shared-access signature (SAS)</a> token in the HTTP Authorization header. SAS tokens are generated from the event hub's URL and the primary key for the policy used to communicate with the event hub — in this case, the policy named "SendPolicy" that you created in the previous exercise.</p>
<p>In this exercise, you will generate a shared-access signature token for the event hub created in <a href="#Exercise1">Exercise 1</a> and copy it, along with the event hub URL, into a Node.js application that will be used to send events to the event hub in Exercise 3. The Azure Portal doesn't provide an interface for generating SAS tokens, so you will generate a token using a Node.js app named sas.js provided with this lab.</p>
<ol>
<li>
<p>If Node.js isn't installed on your computer, go to <a href="https://nodejs.org">https://nodejs.org</a> and install it now.</p>
<blockquote>
<p>You can find out whether Node.js is installed on your computer by executing a <strong>node -v</strong> command in a Command Prompt window or terminal window. If Node.js is installed, you will see the Node.js version number.</p>
</blockquote>
</li>
<li>
<p>Open a Command Prompt window or terminal window and in it, navigate to this lab's "resources" directory. Then execute the following commands to install the packages used by the Node.js apps in the "resources" directory:</p>
<pre><code>npm install shared-access-signature
npm install request
</code></pre>
<blockquote>
<p>It is very important that you run these commands from the lab's "resources" directory so the packages will be installed in the same directory as the apps that use them.</p>
</blockquote>
</li>
<li>
<p>Now execute the following command to run sas.js:</p>
<pre><code>node sas.js
</code></pre>
</li>
<li>
<p>When prompted for the event-hub URL, enter the text below, replacing <em>namespace</em> with the namespace name you entered in Exercise 1, Step 3. Then press Enter.</p>
 <pre> https://<i>namespace</i>.servicebus.windows.net/inputhub</pre>
<blockquote>
<p>This assumes that you named the event hub "inputhub" as directed in Exercise 1, Step 7. If you chose another name, substitute that name for "inputhub" in the event-hub URL.</p>
</blockquote>
</li>
<li>
<p>When prompted, enter the name of the policy (SendPolicy) you created for the Azure event hub in Exercise 1, Step 10. Then press Enter.</p>
</li>
<li>
<p>When prompted, enter the policy key that you saved in Exercise 1, Step 12. Then press Enter.</p>
</li>
<li>
<p>The SAS token, which is highlighted with the red box below, will be output to the Command Prompt or terminal window. Select it and copy it to the clipboard.</p>
<p><a href="Images/copy-sas.png" target="_blank"><img src="Images/copy-sas.png" alt="Copying the SAS token" style="max-width:100%;"></a></p>
<p><em>Copying the SAS token</em></p>
</li>
<li>
<p>Find the file named eventgen.js in the "resources" directory of this lab and open it in your favorite text editor. Then find the section at the top of the file labeled "KEY VARS:"</p>
<div class="highlight highlight-source-js"><pre><span class="pl-c"><span class="pl-c">//</span>/////////////// KEY VARS /////////////////</span>
<span class="pl-k">var</span> sas <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>Token<span class="pl-pds">"</span></span>;
<span class="pl-k">var</span> uri <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>URL<span class="pl-pds">"</span></span>;
<span class="pl-c"><span class="pl-c">//</span>/////////////////////////////////////////</span></pre></div>
</li>
<li>
<p>Replace <em>Token</em> with the SAS token you copied to the clipboard in Step 7. <strong>Important:</strong> The SAS token must <strong>not include line breaks</strong>. It needs to appear on this line as one contiguous string, and it must begin and end with quotation marks. In addition, the line must end with a semicolon.</p>
</li>
<li>
<p>Replace <em>URL</em> with the event-hub URL you entered in Step 4.</p>
</li>
<li>
<p>Save the modified eventgen.js file. The modified "KEY VARS" section should look something like this:</p>
<div class="highlight highlight-source-js"><pre><span class="pl-c"><span class="pl-c">//</span>/////////////// KEY VARS /////////////////</span>
<span class="pl-k">var</span> sas <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>SharedAccessSignature sr=https%3A%2F%2Fstreamanalytics-lab.servicebus.windows.net%2Finputhub&amp;sig=Rz5dVs73XQkUU8KUcrLivDU4Q7%2Bg8zogdApBZHak480%3D&amp;se=1515170996.004&amp;skn=SendPolicy<span class="pl-pds">"</span></span>;
<span class="pl-k">var</span> uri <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>https://streamanalytics-lab.servicebus.windows.net/inputhub<span class="pl-pds">"</span></span>;
<span class="pl-c"><span class="pl-c">//</span>/////////////////////////////////////////</span></pre></div>
</li>
</ol>
<p>Now that you've modified eventgen.js with information specific to your event hub, it's time to generate some events.</p>
<p><a name="Exercise3"></a></p>
<h2>Exercise 3: Send events to the event hub</h2>
<p>In this exercise, you will send events to the event hub you created in <a href="#Exercise1">Exercise 1</a>. To do that, you'll use Node.js to run eventgen.js, which in turn transmits secure requests to the event hub using the <a href="https://msdn.microsoft.com/en-us/library/azure/Dn790674.aspx">Azure Event Hubs REST API</a>. eventgen.js generates events representing withdrawals from simulated ATM machines. Each event contains relevant information such as the card number used for the withdrawal, the time and amount of the withdrawal, and a unique identifier for the ATM machine used.</p>
<ol>
<li>
<p>At the command prompt or in a terminal window, navigate to the "resources" directory of this lab if you aren't there already. Then execute the following command:</p>
<pre><code>node eventgen.js
</code></pre>
<p>You should see output similar to the following. Each line represents one event sent to the event hub, and events will probably roll by at a rate of about 2 to 3 per second. (Rates will vary depending on your connection speed.) <strong>Confirm that each request returns the HTTP status code 201</strong>. This indicates that the event hub received and accepted the request. If you receive any other status code — for example, 401 — then the SAS token probably isn't valid and you need to repeat <a href="#Exercise2">Exercise 2</a>.</p>
<pre><code>[1000] Event sent (status code: 201)
[1001] Event sent (status code: 201)
[1002] Event sent (status code: 201)
[1003] Event sent (status code: 201)
[1004] Event sent (status code: 201)
[1005] Event sent (status code: 201)
[1006] Event sent (status code: 201)
[1007] Event sent (status code: 201)
[1008] Event sent (status code: 201)
[1009] Event sent (status code: 201)
</code></pre>
</li>
<li>
<p>After 10 to 20 events have been sent, press Ctrl+C (or whatever key combination your operating system supports for terminating an application running in a terminal window) to stop the flow of events. <strong>Leave the Command Prompt or terminal window open so you can return to it later.</strong></p>
</li>
</ol>
<p>Now that events are flowing to your event hub, the next step is to create a Stream Analytics job and connect it to the event hub.</p>
<p><a name="Exercise4"></a></p>
<h2>Exercise 4: Create a Stream Analytics job</h2>
<p>In this exercise, you will use the Azure Portal to create a Stream Analytics job and connect it to the event hub you created in <a href="#Exercise1">Exercise 1</a>. You will also capture the raw data being passed to the Stream Analytics job from the event hub and examine its structure.</p>
<ol>
<li>
<p>Return to the <a href="https://portal.azure.com">Azure Portal</a> and click <strong>+ New</strong>, followed by <strong>Internet of Things</strong> and <strong>Stream Analytics job</strong>.</p>
<p><a href="Images/new-stream-analytics-job.png" target="_blank"><img src="Images/new-stream-analytics-job.png" alt="Creating a Stream Analytics job" style="max-width:100%;"></a></p>
<p><em>Creating a Stream Analytics job</em></p>
</li>
<li>
<p>Type "ATMAnalytics" (without quotation marks) into the <strong>Job name</strong> box. Select <strong>Use Existing</strong> under <strong>Resource group</strong> and select the "StreamAnalyticsResourceGroup" resource group that you created in <a href="#Exercise1">Exercise 1</a>. Select the region nearest you for <strong>Location</strong>. (It is important to select the same region that you selected for the event hub in Exercise 1, because you're not charged for data that moves within a data center, but you typically <em>are</em> charged for data that moves <em>between</em> data centers. In addition, locating services that talk to each other in the same data center reduces latency.) Then click the <strong>Create</strong> button.</p>
<p><a href="Images/create-stream-analytics-job.png" target="_blank"><img src="Images/create-stream-analytics-job.png" alt="Specifying parameters for the Stream Analytics job" style="max-width:100%;"></a></p>
<p><em>Specifying parameters for the Stream Analytics job</em></p>
</li>
<li>
<p>Click <strong>Resource groups</strong> in the ribbon on the left, and then click the "StreamAnalyticsResourceGroup" resource group.</p>
<p><a href="Images/open-resource-group.png" target="_blank"><img src="Images/open-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Click <strong>ATMAnalytics</strong> to open the Stream Analytics job in the portal.</p>
<p><a href="Images/open-stream-analytics-job.png" target="_blank"><img src="Images/open-stream-analytics-job.png" alt="Opening the Stream Analytics job" style="max-width:100%;"></a></p>
<p><em>Opening the Stream Analytics job</em></p>
</li>
<li>
<p>Click <strong>Inputs</strong> to add an input to the Stream Analytics job.</p>
<p><a href="Images/add-input-1.png" target="_blank"><img src="Images/add-input-1.png" alt="Adding an input" style="max-width:100%;"></a></p>
<p><em>Adding an input</em></p>
</li>
<li>
<p>Click <strong>+ Add</strong>.</p>
<p><a href="Images/add-input-2.png" target="_blank"><img src="Images/add-input-2.png" alt="Adding an input" style="max-width:100%;"></a></p>
<p><em>Adding an input</em></p>
</li>
<li>
<p>Type "Withdrawals" (without quotation marks) into the <strong>Input alias</strong> box. Make sure <strong>Source Type</strong> is set to <strong>Data stream</strong> and <strong>Source</strong> is set to <strong>Event hub</strong>. Also make sure <strong>Subscription</strong> is set to <strong>Use event hub from current subscription</strong>, <strong>Service bus namespace</strong> is set to the namespace you specified in Exercise 1, Step 3, and the event hub you created in Exercise 1 ("inputhub") is selected under <strong>Event hub name</strong>. Then select <strong>RootManageSharedAccessKey</strong> from the <strong>Event hub policy name</strong> drop-down and click the <strong>Create</strong> button at the bottom of the blade.</p>
<p><a href="Images/create-input.png" target="_blank"><img src="Images/create-input.png" alt="Creating an input" style="max-width:100%;"></a></p>
<p><em>Creating an input</em></p>
</li>
<li>
<p>After a few moments, the new input — "Withdrawals" — appears in the list of inputs for the Stream Analytics job. Click it to open a blade for it.</p>
<p><a href="Images/open-input.png" target="_blank"><img src="Images/open-input.png" alt="Opening the input" style="max-width:100%;"></a></p>
<p><em>Opening the input</em></p>
</li>
<li>
<p>Go back to the Command Prompt or terminal window you left open at the end of the previous exercise and run eventgen.js again by executing the following command:</p>
<pre><code>node eventgen.js
</code></pre>
</li>
<li>
<p>With eventgen.js still running, return to the Azure Portal open in your browser and click <strong>Sample Data</strong>.</p>
<p><a href="Images/sample-data-1.png" target="_blank"><img src="Images/sample-data-1.png" alt="Sampling input data" style="max-width:100%;"></a></p>
<p><em>Sampling input data</em></p>
</li>
<li>
<p>Click <strong>OK</strong> to begin sampling data from the input stream.</p>
<p><a href="Images/sample-data-2.png" target="_blank"><img src="Images/sample-data-2.png" alt="Specifying sampling parameters" style="max-width:100%;"></a></p>
<p><em>Specifying sampling parameters</em></p>
</li>
<li>
<p>Wait a few seconds for sampling to complete, and when you are notified that the sample data can be downloaded, click to download it.</p>
<p><a href="Images/sample-data-3.png" target="_blank"><img src="Images/sample-data-3.png" alt="Data sampling completed" style="max-width:100%;"></a></p>
<p><em>Data sampling completed</em></p>
</li>
<li>
<p>Click <strong>Download</strong> to download the data sampled from the input stream.</p>
<p><a href="Images/sample-data-4.png" target="_blank"><img src="Images/sample-data-4.png" alt="Downloading sample data" style="max-width:100%;"></a></p>
<p><em>Downloading sample data</em></p>
</li>
<li>
<p>Save the JSON file that is downloaded to a location where you can easily find it. Then open the downloaded file in your favorite text editor and take a moment to examine its contents. How many rows (events) are represented in the sample data? What is the structure of each row — that is, what fields does each row contain?</p>
<blockquote>
<p>If you find the output hard to digest since there are no line breaks, try pasting it into an online JSON viewer such as the one at <a href="https://jsonformatter.curiousconcept.com/">https://jsonformatter.curiousconcept.com/</a>.</p>
</blockquote>
</li>
<li>
<p>Return to the Command Prompt or terminal window in which eventgen.js is running and press Ctrl+C (or the equivalent) to stop it.</p>
</li>
</ol>
<p>You have connected a Stream Analytics job to an event hub and demonstrated that data is passed from one to the other. You have also sampled the data input to the Stream Analytics job and examined its structure. The next step is to do something with it — specifically, to bring the power of Azure Stream Analytics to bear on the data.</p>
<p><a name="Exercise5"></a></p>
<h2>Exercise 5: Prepare queries and test with sample data</h2>
<p>Now that your job is set up, there's much more you can do with Stream Analytics than simply view the raw data presented to it. The whole point of Stream Analytics is being able to query the data in real time. In this exercise, you'll use the <a href="https://msdn.microsoft.com/en-us/library/azure/Dn834998.aspx">Stream Analytics Query Language</a> to query a sample data set for potentially fraudulent ATM transactions. It is always a good idea to test your queries against sample data before deploying them against live data streams, because with sample data, you can verify that a known set of inputs produces the expected outputs.</p>
<p>To flag potentially fraudulent withdrawals from ATMs, you will query for transactions performed with the same ATM card at different ATM machines within a specified time window (60 seconds). In real life, you would probably use a larger time window and perhaps even factor in the distance between ATM machines. However, a narrower time window is useful in a lab environment because it allows you to perform meaningful experiments in minutes rather than hours.</p>
<ol>
<li>
<p>Begin by returning to the Stream Analytics job in the portal and clicking <strong>Query</strong>.</p>
<p><a href="Images/add-query.png" target="_blank"><img src="Images/add-query.png" alt="Opening the query viewer" style="max-width:100%;"></a></p>
<p><em>Opening the query viewer</em></p>
</li>
<li>
<p>Click the <strong>ellipsis</strong> (the three dots) to the right of <strong>Withdrawals</strong> and select <strong>Upload sample data from file</strong> from the menu.</p>
<p><a href="Images/upload-test-data-1.png" target="_blank"><img src="Images/upload-test-data-1.png" alt="Uploading sample data for testing queries" style="max-width:100%;"></a></p>
<p><em>Uploading sample data for testing queries</em></p>
</li>
<li>
<p>Click the <strong>folder</strong> icon on the right and select the file named <strong>Withdrawals.json</strong> in this lab's "resources" directory. Then click <strong>OK</strong> to upload the file.</p>
<blockquote>
<p>The reason you're using a file provided for you (rather than the one you captured in the previous exercise) is to make sure everyone gets the same results. eventgen.js uses JavaScript's Math.random() function to randomize results, and Math.random() does not produce repeatable sequences of pseudo-random numbers.</p>
</blockquote>
<p><a href="Images/upload-test-data-2.png" target="_blank"><img src="Images/upload-test-data-2.png" alt="Uploading Withdrawals.json" style="max-width:100%;"></a></p>
<p><em>Uploading Withdrawals.json</em></p>
</li>
<li>
<p>When the upload is complete, enter the following query, and then click the <strong>Test</strong> button to execute it against the sample data you uploaded:</p>
<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> <span class="pl-k">*</span> <span class="pl-k">FROM</span> Withdrawals</pre></div>
<blockquote>
<p>Where did the name "Withdrawals" come from? That's the alias you assigned to the event-hub input in the previous exercise. If you named it differently, you'll need to replace "Withdrawals" with the alias you used.</p>
</blockquote>
<p><a href="Images/test-query-1.png" target="_blank"><img src="Images/test-query-1.png" alt="Testing a query" style="max-width:100%;"></a></p>
<p><em>Testing a query</em></p>
</li>
<li>
<p>Confirm that you see the output pictured below. The test data contains 607 rows. Each row has fields named TRANSACTIONID, TRANSACTIONTIME, DEVICEID, CARDNUMBER, and AMOUNT. DEVICEID is the ID of the ATM machine at which the transaction took place. AMOUNT is the amount of cash withdrawn from the ATM.</p>
<p><a href="Images/query-results-1.png" target="_blank"><img src="Images/query-results-1.png" alt="Query results" style="max-width:100%;"></a></p>
<p><em>Query results</em></p>
</li>
<li>
<p>Suppose you only wanted to view transactions for amounts between 200 and 300, inclusive. Furthermore, suppose you wanted to clean up the output by assigning your own column names and excluding the TRANSACTIONID column. Enter the following query and click <strong>Test</strong> again to execute it.</p>
<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> TransactionTime <span class="pl-k">as</span> [<span class="pl-k">Time</span> of Transaction],
       DeviceID <span class="pl-k">as</span> [ATM],
       CardNumber <span class="pl-k">as</span> [Card <span class="pl-k">Number</span>],
       Amount <span class="pl-k">as</span> [Amount]
<span class="pl-k">FROM</span> Withdrawals
<span class="pl-k">WHERE</span> Amount <span class="pl-k">&gt;=</span> <span class="pl-c1">200</span> <span class="pl-k">and</span> Amount <span class="pl-k">&lt;=</span> <span class="pl-c1">300</span></pre></div>
</li>
<li>
<p>Confirm that the query generated the following output:</p>
<p><a href="Images/query-results-2.png" target="_blank"><img src="Images/query-results-2.png" alt="Customizing the output" style="max-width:100%;"></a></p>
<p><em>Customizing the output</em></p>
</li>
<li>
<p>One of the key features of the Stream Analytics Query Language is its ability to group results using windows of time whose length you specify. To demonstrate, enter the following query to count the number of transactions taking place each minute and click <strong>Test</strong> to execute it:</p>
<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> <span class="pl-c1">System</span>.<span class="pl-c1">Timestamp</span> <span class="pl-k">as</span> [<span class="pl-k">Time</span> Ending],
    <span class="pl-c1">COUNT</span>(<span class="pl-k">*</span>) <span class="pl-k">AS</span> [<span class="pl-k">Number</span> of Transactions]
<span class="pl-k">FROM</span> Withdrawals <span class="pl-k">TIMESTAMP</span> BY TransactionTime
<span class="pl-k">GROUP BY</span> TumblingWindow(n, <span class="pl-c1">1</span>)</pre></div>
<blockquote>
<p>TIMESTAMP BY is an important element of the Stream Analytics Query Language. If it was omitted from the query above, you would be querying for the number of transactions that arrived <em>at the event hub</em> each minute rather than the number of transactions that occurred in each 1-minute interval. TIMESTAMP BY allows you to specify a field in the input stream as the event time.</p>
</blockquote>
</li>
<li>
<p>Confirm that you see the output below:</p>
<p><a href="Images/query-results-3.png" target="_blank"><img src="Images/query-results-3.png" alt="Querying for the number of transactions per minute" style="max-width:100%;"></a></p>
<p><em>Querying for the number of transactions per minute</em></p>
</li>
<li>
<p>Now it's time to query the test data for potentially fraudulent transactions — transactions involving the same ATM card but different ATM machines that take place within 60 seconds of each other. <em>This is the query you will use in the next exercise against a live data stream</em>.</p>
<p>Enter the following query and click <strong>Test</strong> to execute it:</p>
<div class="highlight highlight-source-sql"><pre><span class="pl-k">SELECT</span> <span class="pl-c1">W1</span>.<span class="pl-c1">CardNumber</span> <span class="pl-k">as</span> [Card <span class="pl-k">Number</span>],
    <span class="pl-c1">W1</span>.<span class="pl-c1">DeviceID</span> <span class="pl-k">as</span> [ATM <span class="pl-c1">1</span>], <span class="pl-c1">W2</span>.<span class="pl-c1">DeviceID</span> <span class="pl-k">as</span> [ATM <span class="pl-c1">2</span>],
    <span class="pl-c1">W1</span>.<span class="pl-c1">TransactionTime</span> <span class="pl-k">as</span> [<span class="pl-k">Time</span> <span class="pl-c1">1</span>], <span class="pl-c1">W2</span>.<span class="pl-c1">TransactionTime</span> <span class="pl-k">as</span> [<span class="pl-k">Time</span> <span class="pl-c1">2</span>]
<span class="pl-k">FROM</span> Withdrawals W1 <span class="pl-k">TIMESTAMP</span> BY TransactionTime
<span class="pl-k">JOIN</span> Withdrawals W2 <span class="pl-k">TIMESTAMP</span> BY TransactionTime
<span class="pl-k">ON</span> <span class="pl-c1">W1</span>.<span class="pl-c1">CardNumber</span> <span class="pl-k">=</span> <span class="pl-c1">W2</span>.<span class="pl-c1">CardNumber</span>
<span class="pl-k">AND</span> DATEDIFF(ss, W1, W2) BETWEEN <span class="pl-c1">0</span> <span class="pl-k">and</span> <span class="pl-c1">60</span>
<span class="pl-k">WHERE</span> <span class="pl-c1">W1</span>.<span class="pl-c1">DeviceID</span> <span class="pl-k">!=</span> <span class="pl-c1">W2</span>.<span class="pl-c1">DeviceID</span></pre></div>
</li>
<li>
<p>This time the output should contain just three rows, each representing two transactions performed with one ATM card at two different locations within 60 seconds of each other:</p>
<p><a href="Images/query-results-4.png" target="_blank"><img src="Images/query-results-4.png" alt="Detecting potentially fraudulent transactions" style="max-width:100%;"></a></p>
<p><em>Detecting potentially fraudulent transactions</em></p>
</li>
<li>
<p>Click the <strong>Save</strong> button at the top of the blade to save the query. Then click <strong>Yes</strong> when asked to confirm.</p>
<p><a href="Images/save-query.png" target="_blank"><img src="Images/save-query.png" alt="Saving the query" style="max-width:100%;"></a></p>
<p><em>Saving the query</em></p>
</li>
</ol>
<p>With the query now formulated, tested against a set of sample data, and saved, it's time to deploy it against a live data stream to produce a running record of potentially fraudulent transactions.</p>
<p><a name="Exercise6"></a></p>
<h2>Exercise 6: Analyze a live data stream</h2>
<p>Being able to run your queries and see the results in the portal is great for testing, but when Azure Stream Analytics is deployed against a live data stream, you need to specify a destination (or destinations) for the output. Stream Analytics supports a variety of output types, including blobs, Azure SQL databases, and even event hubs. One motivation for using blob storage is to create a persistent record from the output.</p>
<p>In this exercise, you will create a storage account and configure the Stream Analytics job to store output in blob storage. Then you will run the job against a live data stream and check the results by inspecting blob storage.</p>
<ol>
<li>
<p>Return to the Azure Portal and click <strong>+ New</strong> in the ribbon on the left. Then click <strong>Storage</strong>, followed by <strong>Storage account</strong>.</p>
<p><a href="Images/new-storage-account.png" target="_blank"><img src="Images/new-storage-account.png" alt="Adding a storage account" style="max-width:100%;"></a></p>
<p><em>Adding a storage account</em></p>
</li>
<li>
<p>In the ensuing blade, enter a name for the new storage account in <strong>Name</strong> field.</p>
<blockquote>
<p>Storage account names must be 3 to 24 characters in length and can only contain numbers and lowercase letters. In addition, the name you enter must be unique within Azure.</p>
</blockquote>
<p>Once you have a unique name that Azure will accept (as indicated by the green check mark in the <strong>Name</strong> field), select <strong>Use existing</strong> under <strong>Resource group</strong> and select the resource group named "StreamAnalyticsResourceGroup" so the storage account will belong to the same resource group as the Stream Analytics job and the event hub. Select the location nearest you (the same one you selected for the event hub in <a href="#Exercise1">Exercise 1</a> and the Stream Analytics job in <a href="#Exercise4">Exercise 4</a>) in the <strong>Location</strong> box. Then click the <strong>Create</strong> button at the bottom of the blade.</p>
<p><a href="Images/create-storage-account.png" target="_blank"><img src="Images/create-storage-account.png" alt="Creating a storage account" style="max-width:100%;"></a></p>
<p><em>Creating a storage account</em></p>
</li>
<li>
<p>Click <strong>Resource groups</strong> in the ribbon on the left, and then click the "StreamAnalyticsResourceGroup" resource group.</p>
<p><a href="Images/open-resource-group.png" target="_blank"><img src="Images/open-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Click <strong>ATMAnalytics</strong> to open the Stream Analytics job in the portal.</p>
<p><a href="Images/open-stream-analytics-job-2.png" target="_blank"><img src="Images/open-stream-analytics-job-2.png" alt="Opening the Stream Analytics job" style="max-width:100%;"></a></p>
<p><em>Opening the Stream Analytics job</em></p>
</li>
<li>
<p>Click <strong>Outputs</strong>.</p>
<p><a href="Images/add-output-1.png" target="_blank"><img src="Images/add-output-1.png" alt="Adding an output" style="max-width:100%;"></a></p>
<p><em>Adding an output</em></p>
</li>
<li>
<p>Click <strong>+ Add</strong>.</p>
<p><a href="Images/add-output-2.png" target="_blank"><img src="Images/add-output-2.png" alt="Adding an output" style="max-width:100%;"></a></p>
<p><em>Adding an output</em></p>
</li>
<li>
<p>Type "FlaggedWithdrawals" (without quotation marks) into the <strong>Output alias</strong> box. Set <strong>Sink</strong> to <strong>Blob storage</strong> and <strong>Subscription</strong> to <strong>Use blob storage from current subscription</strong>. Under <strong>Storage account</strong>, select the storage account you created earlier in this exercise. Set <strong>Container</strong> to <strong>Create a new container</strong> and type "output" (without quotation marks) into the second <strong>Container</strong> box. Type "{date}" (without quotation marks) into the <strong>Path pattern</strong> box, and set <strong>Date format</strong> to <strong>DD-MM-YYYY</strong>. Then click <strong>Create</strong>.</p>
<blockquote>
<p>Each time you run a Stream Analytics job configured with a blob output, a new blob with a unique name is created. The purpose of <strong>Path pattern</strong> is to allow you to organize output blobs by date and time. In this example, the "output" container will contain folders whose names are the dates of the runs, and each folder will contain blobs with the output from those runs.</p>
</blockquote>
<p><a href="Images/create-output.png" target="_blank"><img src="Images/create-output.png" alt="Creating an output" style="max-width:100%;"></a></p>
<p><em>Creating an output</em></p>
</li>
<li>
<p>Close the "Outputs" blade and return to the blade for the Stream Analytics job. Then click <strong>Start</strong>.</p>
<p><a href="Images/start-stream-analytics-job-1.png" target="_blank"><img src="Images/start-stream-analytics-job-1.png" alt="Starting the Stream Analytics job" style="max-width:100%;"></a></p>
<p><em>Starting the Stream Analytics job</em></p>
</li>
<li>
<p>Make sure <strong>Job output start time</strong> is set to <strong>Now</strong>, and then click the <strong>Start</strong> button to start running the job.</p>
<p><a href="Images/start-stream-analytics-job-2.png" target="_blank"><img src="Images/start-stream-analytics-job-2.png" alt="Specifying the job start time" style="max-width:100%;"></a></p>
<p><em>Specifying the job start time</em></p>
</li>
<li>
<p>Return to the Command Prompt or terminal window in which you ran eventgen.js and execute the following command to run it again:</p>
<pre><code>node eventgen.js
</code></pre>
</li>
<li>
<p>Wait 5 minutes or more to give the job time to start and eventgen.js time to transmit several hundred events. Then terminate eventgen.js and return to the browser window.</p>
<blockquote>
<p>If you'd like, you can open several terminal windows and run eventgen.js in each one to increase the volume of events.</p>
</blockquote>
</li>
<li>
<p>Return to the Stream Analytics job in the portal and click <strong>Stop</strong> to stop it. Then click <strong>Yes</strong> when asked to confirm that you want to stop the job.</p>
<p><a href="Images/stop-stream-analytics-job.png" target="_blank"><img src="Images/stop-stream-analytics-job.png" alt="Stopping the Stream Analytics job" style="max-width:100%;"></a></p>
<p><em>Stopping the Stream Analytics job</em></p>
</li>
<li>
<p>Wait until the job is stopped. Then open the blade for the "StreamAnalyticsResourceGroup" resource group and click the storage account you created in Step 2.</p>
<p><a href="Images/open-storage-account.png" target="_blank"><img src="Images/open-storage-account.png" alt="Opening the storage account" style="max-width:100%;"></a></p>
<p><em>Opening the storage account</em></p>
</li>
<li>
<p>Click <strong>Blobs</strong>.</p>
<p><a href="Images/open-blob-storage.png" target="_blank"><img src="Images/open-blob-storage.png" alt="Opening blob storage" style="max-width:100%;"></a></p>
<p><em>Opening blob storage</em></p>
</li>
<li>
<p>Click the container named "output."</p>
<p><a href="Images/open-container.png" target="_blank"><img src="Images/open-container.png" alt="Opening the output container" style="max-width:100%;"></a></p>
<p><em>Opening the output container</em></p>
</li>
<li>
<p>Click the folder in the "output" container.</p>
<p><a href="Images/open-folder.png" target="_blank"><img src="Images/open-folder.png" alt="Opening the output folder" style="max-width:100%;"></a></p>
<p><em>Opening the output folder</em></p>
</li>
<li>
<p>Click the blob containing the Stream Analytics output.</p>
<blockquote>
<p>If there is no output blob, wait a few minutes and check again. Sometimes a blob created by a Stream Analytics job appears immediately, and at other times, it may take a few minutes to show up.</p>
</blockquote>
<p><a href="Images/open-blob.png" target="_blank"><img src="Images/open-blob.png" alt="Opening the output blob" style="max-width:100%;"></a></p>
<p><em>Opening the output blob</em></p>
</li>
<li>
<p>Click <strong>Download</strong> to download the blob.</p>
<p><a href="Images/download-blob.png" target="_blank"><img src="Images/download-blob.png" alt="Downloading the output blob" style="max-width:100%;"></a></p>
<p><em>Downloading the output blob</em></p>
</li>
<li>
<p>Open the downloaded JSON file in your favorite text editor. Each object (row) in the output represents a potentially fraudulent transaction. Note that <strong>the number of rows and the content of each row will vary from machine to machine as well as from one run to another</strong>.</p>
<p><a href="Images/output-blob.png" target="_blank"><img src="Images/output-blob.png" alt="JSON job output" style="max-width:100%;"></a></p>
<p><em>JSON job output</em></p>
</li>
</ol>
<p>Currently, the output from your Stream Analytics job is stored in blobs. In real life, you might prefer to view the output in a more convenient form, such as in a chart that's updated in real time. You could accomplish that by writing an application that monitors the blob and charts the data, or, better yet, by directing the output to an event hub and writing an application that subscribes to events from the event hub.</p>
<p>Microsoft recognizes that not everyone wants to write applications, and has provided an alternative in the form of <a href="https://powerbi.microsoft.com/">Microsoft Power BI</a>. With Power BI, you can create dashboards that render output from Stream Analytics jobs without writing any code. For more information, refer to <a href="https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-power-bi-dashboard/">Stream Analytics &amp; Power BI: A real-time analytics dashboard for streaming data</a>.</p>
<p><a name="Summary"></a></p>
<h2>Summary</h2>
<p>Azure Stream Analytics is a powerful tool for analyzing live data streams from IoT devices or anything else that's capable of transmitting data. In this lab, you got a first-hand look at Stream Analytics as well as Azure event hubs. Among other things, you learned how to:</p>
<ul>
<li>Create an Azure event hub and use it as a Stream Analytics input</li>
<li>Create a shared-access signature that allows event hubs to be called securely using REST APIs</li>
<li>Create a Stream Analytics job and test queries on sample data streams</li>
<li>Run a Stream Analytics job and perform queries on live data streams</li>
<li>Create a rule (query) that detects anomalies in streaming data</li>
<li>Use that rule to record anomalies in Azure blobs</li>
</ul>
<p>One drawback to hard-coding rules into Stream Analytics is that rules don't "learn" from the data streams, which can lead to false positives in anomaly detection. If this concerns you, read the article entitled <a href="http://blogs.technet.com/b/machinelearning/archive/2014/11/05/anomaly-detection-using-machine-learning-to-detect-abnormalities-in-time-series-data.aspx">Anomaly Detection – Using Machine Learning to Detect Abnormalities in Time Series Data</a> in the Azure team's Machine Learning blog. In it, they present an anomaly detection service accessible through a REST API that uses Azure Machine Learning to learn from the data presented to it. Imagine combining the power of Stream Analytics to extract information from real-time data streams with the power of Azure Machine Learning to learn from that information and refine the analytics on the fly. This is precisely the type of solution that Microsoft Azure empowers you to build!</p>
<hr>
<p>Copyright 2017 Microsoft Corporation. All rights reserved. Except where otherwise noted, these materials are licensed under the terms of the MIT License. You may use them according to the license as is most appropriate for your project. The terms of this license can be found at <a href="https://opensource.org/licenses/MIT">https://opensource.org/licenses/MIT</a>.</p>
</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
