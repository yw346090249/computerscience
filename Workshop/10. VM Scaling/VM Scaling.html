<!DOCTYPE html>
<html>
<head>
<title>VM Scaling</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
<style type="text/css">
.highlight  { background: #ffffff; }
.highlight .c { color: #999988; font-style: italic } /* Comment */
.highlight .err { color: #a61717; background-color: #e3d2d2 } /* Error */
.highlight .k { font-weight: bold } /* Keyword */
.highlight .o { font-weight: bold } /* Operator */
.highlight .cm { color: #999988; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #999999; font-weight: bold } /* Comment.Preproc */
.highlight .c1 { color: #999988; font-style: italic } /* Comment.Single */
.highlight .cs { color: #999999; font-weight: bold; font-style: italic } /* Comment.Special */
.highlight .gd { color: #000000; background-color: #ffdddd } /* Generic.Deleted */
.highlight .gd .x { color: #000000; background-color: #ffaaaa } /* Generic.Deleted.Specific */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #aa0000 } /* Generic.Error */
.highlight .gh { color: #999999 } /* Generic.Heading */
.highlight .gi { color: #000000; background-color: #ddffdd } /* Generic.Inserted */
.highlight .gi .x { color: #000000; background-color: #aaffaa } /* Generic.Inserted.Specific */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #555555 } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #aaaaaa } /* Generic.Subheading */
.highlight .gt { color: #aa0000 } /* Generic.Traceback */
.highlight .kc { font-weight: bold } /* Keyword.Constant */
.highlight .kd { font-weight: bold } /* Keyword.Declaration */
.highlight .kp { font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #445588; font-weight: bold } /* Keyword.Type */
.highlight .m { color: #009999 } /* Literal.Number */
.highlight .s { color: #d14 } /* Literal.String */
.highlight .na { color: #008080 } /* Name.Attribute */
.highlight .nb { color: #0086B3 } /* Name.Builtin */
.highlight .nc { color: #445588; font-weight: bold } /* Name.Class */
.highlight .no { color: #008080 } /* Name.Constant */
.highlight .ni { color: #800080 } /* Name.Entity */
.highlight .ne { color: #990000; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #990000; font-weight: bold } /* Name.Function */
.highlight .nn { color: #555555 } /* Name.Namespace */
.highlight .nt { color: #000080 } /* Name.Tag */
.highlight .nv { color: #008080 } /* Name.Variable */
.highlight .ow { font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #009999 } /* Literal.Number.Float */
.highlight .mh { color: #009999 } /* Literal.Number.Hex */
.highlight .mi { color: #009999 } /* Literal.Number.Integer */
.highlight .mo { color: #009999 } /* Literal.Number.Oct */
.highlight .sb { color: #d14 } /* Literal.String.Backtick */
.highlight .sc { color: #d14 } /* Literal.String.Char */
.highlight .sd { color: #d14 } /* Literal.String.Doc */
.highlight .s2 { color: #d14 } /* Literal.String.Double */
.highlight .se { color: #d14 } /* Literal.String.Escape */
.highlight .sh { color: #d14 } /* Literal.String.Heredoc */
.highlight .si { color: #d14 } /* Literal.String.Interpol */
.highlight .sx { color: #d14 } /* Literal.String.Other */
.highlight .sr { color: #009926 } /* Literal.String.Regex */
.highlight .s1 { color: #d14 } /* Literal.String.Single */
.highlight .ss { color: #990073 } /* Literal.String.Symbol */
.highlight .bp { color: #999999 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #008080 } /* Name.Variable.Class */
.highlight .vg { color: #008080 } /* Name.Variable.Global */
.highlight .vi { color: #008080 } /* Name.Variable.Instance */
.highlight .il { color: #009999 } /* Literal.Number.Integer.Long */
.pl-c {
    color: #969896;
}

.pl-c1,.pl-mdh,.pl-mm,.pl-mp,.pl-mr,.pl-s1 .pl-v,.pl-s3,.pl-sc,.pl-sv {
    color: #0086b3;
}

.pl-e,.pl-en {
    color: #795da3;
}

.pl-s1 .pl-s2,.pl-smi,.pl-smp,.pl-stj,.pl-vo,.pl-vpf {
    color: #333;
}

.pl-ent {
    color: #63a35c;
}

.pl-k,.pl-s,.pl-st {
    color: #a71d5d;
}

.pl-pds,.pl-s1,.pl-s1 .pl-pse .pl-s2,.pl-sr,.pl-sr .pl-cce,.pl-sr .pl-sra,.pl-sr .pl-sre,.pl-src,.pl-v {
    color: #df5000;
}

.pl-id {
    color: #b52a1d;
}

.pl-ii {
    background-color: #b52a1d;
    color: #f8f8f8;
}

.pl-sr .pl-cce {
    color: #63a35c;
    font-weight: bold;
}

.pl-ml {
    color: #693a17;
}

.pl-mh,.pl-mh .pl-en,.pl-ms {
    color: #1d3e81;
    font-weight: bold;
}

.pl-mq {
    color: #008080;
}

.pl-mi {
    color: #333;
    font-style: italic;
}

.pl-mb {
    color: #333;
    font-weight: bold;
}

.pl-md,.pl-mdhf {
    background-color: #ffecec;
    color: #bd2c00;
}

.pl-mdht,.pl-mi1 {
    background-color: #eaffea;
    color: #55a532;
}

.pl-mdr {
    color: #795da3;
    font-weight: bold;
}

.pl-mo {
    color: #1d3e81;
}
.task-list {
padding-left:10px;
margin-bottom:0;
}

.task-list li {
    margin-left: 20px;
}

.task-list-item {
list-style-type:none;
padding-left:10px;
}

.task-list-item label {
font-weight:400;
}

.task-list-item.enabled label {
cursor:pointer;
}

.task-list-item+.task-list-item {
margin-top:3px;
}

.task-list-item-checkbox {
display:inline-block;
margin-left:-20px;
margin-right:3px;
vertical-align:1px;
}
</style>
</head>
<body>
<p><a name="HOLTitle"></a></p>
<h1>Virtual Machine Scaling and Performance</h1>
<hr>
<p><a name="Overview"></a></p>
<h2>Overview</h2>
<p>One of the benefits of using the cloud to handle large computing workloads is virtually limitless scalability. In Microsoft Azure, you can create a cluster of virtual machines (VMs) networked to form a high-performance computing (HPC) cluster in a matter of minutes. If you need more computing power than the cluster can provide, you can <em>scale up</em> by creating a cluster with larger and more capable virtual machines (more cores, more RAM, etc.), or you can <em>scale out</em> by creating a cluster with more nodes. Finding the optimum cluster configuration for a given job is a key requirement for utilizing compute resources effectively and efficiently.</p>
<p>In this hands-on lab, you will run a compute-intensive job on three different Linux HPC clusters and compare performance. The cluster sizes you will use are:</p>
<ul>
<li>One worker node with one core and 3.5 GB of RAM</li>
<li>One worker node with eight cores and 28 GB of RAM</li>
<li>Eight worker nodes, each with one core and 3.5 GB of RAM</li>
</ul>
<p>The job that you will run is written in Python, and it involves using the latitudes and longitudes of more than 7,300 airports to build a distance table like the one below containing the distance between each airport. Since the number of distances is the square of the number of airports, the job computes more than 53 million distances, and the calculations are floating-point intensive as well as CPU-intensive because the distances are calculated over the <a href="https://en.wikipedia.org/wiki/Great-circle_distance">surface of a sphere</a>.</p>
<p><a href="Images/disttabl.gif" target="_blank"><img src="Images/disttabl.gif" alt="Sample Distance Table" style="max-width:100%;"></a></p>
<p><em>Sample distance table</em></p>
<p>To distribute the workload among all the nodes and cores in each cluster, the Python code that you will run uses the <a href="https://slurm.schedmd.com/">Simple Linux Utility for Resource Management</a>, also known as the SLURM Workload Manager or simply SLURM. SLURM is a free and open-source job scheduler for Linux that excels at distributing heavy computing workloads across clusters of machines and processors. It is used on more than half of the world's largest supercomputers and HPC clusters, and it enjoys widespread use in the research community for jobs that require significant compute resources.</p>
<p>This lab normally requires 90 minutes or more to complete because it has you deploy and test three clusters sequentially. You can economize on time by deploying all three clusters in parallel, essentially working Exercises 2, 7, and 8 at the same time. Be aware, however, that some free Azure subscriptions limit the number of virtual-machine cores that can be extant at any one time, so depending on what type of subscription you have, you may or may not be able to deploy three clusters concurrently.</p>
<p><a name="Objectives"></a></p>
<h3>Objectives</h3>
<p>In this hands-on lab, you will learn how to:</p>
<ul>
<li>Create a SLURM cluster in Azure</li>
<li>Copy local resources to a SLURM cluster</li>
<li>Remote into a SLURM cluster</li>
<li>Run jobs on a SLURM cluster</li>
<li>Use the Azure Resource Manager to delete a SLURM cluster</li>
</ul>
<p>Moreover, you will get a first-hand look at the differences in performance when scaling up versus scaling out. Of course, every job is different, so the exercises herein provide just one example of the performance differences one can expect.</p>
<p><a name="Prerequisites"></a></p>
<h3>Prerequisites</h3>
<p>The following are required to complete this hands-on lab:</p>
<ul>
<li>An active Microsoft Azure subscription, or <a href="https://azure.microsoft.com/en-us/free/">sign up for a free trial</a></li>
<li><a href="http://www.chiark.greenend.org.uk/%7Esgtatham/putty/download.html">PuTTY</a> (Windows users only)</li>
</ul>
<p><a name="Exercises"></a></p>
<h2>Exercises</h2>
<ul>
<li><a href="#Exercise1">Exercise 1: Create a storage account and configure Python scripts</a></li>
<li><a href="#Exercise2">Exercise 2: Deploy an HPC cluster</a></li>
<li><a href="#Exercise3">Exercise 3 (macOS and Linux): Connect to and configure the cluster</a></li>
<li><a href="#Exercise4">Exercise 4 (Windows): Connect to and configure the cluster</a></li>
<li><a href="#Exercise5">Exercise 5: Run a job and view the results</a></li>
<li><a href="#Exercise6">Exercise 6: Delete the cluster</a></li>
<li><a href="#Exercise7">Exercise 7: Test with a cluster containing one worker nodes with eight cores</a></li>
<li><a href="#Exercise8">Exercise 8: Test with a cluster containing eight worker nodes with one core each</a></li>
<li><a href="#Exercise9">Exercise 9: Delete the storage account</a></li>
</ul>
<p>Estimated time to complete this lab: <strong>90 minutes</strong>.</p>
<p><a name="Exercise1"></a></p>
<h2>Exercise 1: Create a storage account and configure Python scripts</h2>
<p>In this exercise, you will use the <a href="https://portal.azure.com">Azure Portal</a> to create a storage account. Then you will modify the job scripts used to compare the performance of various VM configurations so they can use this account to store data in blob storage.</p>
<ol>
<li>
<p>Open the <a href="https://portal.azure.com">Azure Portal</a> in your browser. If you are asked to log in, do so using your Microsoft account.</p>
</li>
<li>
<p>Click <strong>+ New</strong> in the ribbon on the left. Then click <strong>Storage</strong>, followed by <strong>Storage account</strong>.</p>
<p><a href="Images/new-storage-account.png" target="_blank"><img src="Images/new-storage-account.png" alt="Creating a storage account" style="max-width:100%;"></a></p>
<p><em>Creating a storage account</em></p>
</li>
<li>
<p>In the ensuing "Create storage account" blade, enter a name for the new storage account in <strong>Name</strong> field. The name is important, because it forms one part of the URL through which blobs created under this account are accessed.</p>
<blockquote>
<p>Storage account names can be 3 to 24 characters in length and can only contain numbers and lowercase letters. In addition, the name you enter must be unique within Azure. If someone else has chosen the same name, you'll be notified that the name isn't available with a red exclamation mark in the <strong>Name</strong> field.</p>
</blockquote>
<p>Once you have a name that Azure will accept (as indicated by the green check mark in the <strong>Name</strong> field), make sure <strong>Resource manager</strong> is selected as the <strong>Deployment model</strong> and <strong>General purpose</strong> is selected as the <strong>Account kind</strong>. Then select <strong>Create new</strong> under <strong>Resource group</strong> and type "ScalingLabResourceGroup" (without quotation marks) into the box below to name the new resource group that will be created for the storage account. Finish up by selecting the location nearest you in the <strong>Location</strong> box, and clicking the <strong>Create</strong> button at the bottom of the blade to create the new storage account.</p>
<p><a href="Images/create-storage-account.png" target="_blank"><img src="Images/create-storage-account.png" alt="Specifying parameters for a new storage account" style="max-width:100%;"></a></p>
<p><em>Specifying parameters for a new storage account</em></p>
</li>
<li>
<p>After the account is created (it generally takes 30 seconds or so), click <strong>Resource groups</strong> in the ribbon on the left side of the portal, and then click the "ScalingLabResourceGroup" resource group that was created along with the storage account.</p>
<p><a href="Images/open-resource-group.png" target="_blank"><img src="Images/open-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Click the storage account that you created in Step 3.</p>
<p><a href="Images/open-storage-account.png" target="_blank"><img src="Images/open-storage-account.png" alt="Opening the storage account" style="max-width:100%;"></a></p>
<p><em>Opening the storage account</em></p>
</li>
<li>
<p>Click <strong>Access keys</strong> to display the storage account's access keys, and then click the <strong>Copy</strong> button to the right of the primary key to copy it to the clipboard.</p>
<p><a href="Images/copy-access-key.png" target="_blank"><img src="Images/copy-access-key.png" alt="Copying the access key" style="max-width:100%;"></a></p>
<p><em>Copying the access key</em></p>
</li>
<li>
<p>Find <strong>controller.py</strong> in this lab's "resources" folder and open it in the text or program editor of your choice.</p>
</li>
<li>
<p>In <strong>controller.py</strong>, replace <em>storage_account_name</em> on line 14 with the name of the storage account you created in Step 3, and replace <em>storage_account_key</em> on line 15 with the access key that is on the clipboard. Then save your changes and close the file.</p>
<p><a href="Images/modify-script.png" target="_blank"><img src="Images/modify-script.png" alt="Modifying controller.py" style="max-width:100%;"></a></p>
<p><em>Modifying controller.py</em></p>
</li>
<li>
<p>Find <strong>worker.py</strong> in this lab's "resources" folder and open it in the text or program editor of your choice.</p>
</li>
<li>
<p>In <strong>worker.py</strong>, replace <em>storage_account_name</em> on line 11 with the name of the storage account you created in Step 3, and replace <em>storage_account_key</em> on line 12 with the access key that is on the clipboard. Then save your changes and close the file.</p>
</li>
</ol>
<p>You now have an Azure storage account that you can use in your tests as well as Python scripts that can access the storage account. The next step is to deploy your first HPC cluster for testing.</p>
<p><a name="Exercise2"></a></p>
<h2>Exercise 2: Deploy an HPC cluster</h2>
<p>The Azure Resource Manager allows you to provision applications using declarative templates. A template contains a complete description of everything that makes up the application, including virtual machines, databases, Web apps, IP addresses, and other resources. Templates can include parameters that users are prompted to fill in each time an application is deployed. Templates can also invoke scripts to initialize resources to a known and consistent state. To learn more about Azure Resource Manager templates, refer to the <a href="https://azure.microsoft.com/en-us/documentation/articles/resource-group-template-deploy/">documentation</a> online.</p>
<p>In this exercise, you will use a deployment template built by the Azure team. This template creates a collection of virtual machines and all the resources required to form a SLURM HPC cluster from them. It is one of many useful templates on the <a href="http://azure.microsoft.com/en-us/documentation/templates/">Azure Quickstart Templates</a> page and in the Quickstart templates <a href="https://github.com/Azure/azure-quickstart-templates">GitHub repository</a>.</p>
<p>The template you will use, which you can <a href="https://github.com/Azure/azure-quickstart-templates/tree/master/slurm">view here</a> on GitHub, is titled "Deploy a slurm cluster." It performs the following steps:</p>
<ul>
<li>Deploys a master VM plus a specified number of worker VMs</li>
<li>Creates a private network for the VMs (nodes) in the cluster</li>
<li>Creates a public IP address for master node</li>
<li>Creates an identical user account on all nodes</li>
<li>Executes a shell script to configure SLURM on all nodes</li>
</ul>
<p>Let's get started!</p>
<ol>
<li>
<p>In your browser, navigate to <a href="https://github.com/Azure/azure-quickstart-templates/tree/master/slurm">https://github.com/Azure/azure-quickstart-templates/tree/master/slurm</a>. In the middle of the page, click the <strong>Deploy to Azure</strong> button. This will load the template into a new instance of the Azure Portal. You may be asked to sign in again. If you are, sign in using your Microsoft account.</p>
<p><a href="Images/deploy-to-azure.png" target="_blank"><img src="Images/deploy-to-azure.png" alt="Deploying from GitHub" style="max-width:100%;"></a></p>
<p><em>Deploying from GitHub</em></p>
</li>
<li>
<p>Select <strong>Create new</strong> under <strong>Resource group</strong> and enter the resource-group name "ClusterResourceGroup" (without quotation marks). It is important NOT to use the same resource group you used for the storage account in Exercise 1, because when you delete the cluster in Exercise 6, you don't want the storage account to be deleted, too.</p>
<p>Select the location nearest you — the same one you selected for the storage account in Exercise 1 — under <strong>Location</strong>. Specify "azureuser" as the <strong>Admin User Name</strong> and "Azure4Research!" as the <strong>Admin Password</strong>. Leave <strong>Vm Size</strong> set to <strong>Standard_D1_v2</strong> and set <strong>Scale Number</strong> to <strong>1</strong> to create a cluster containing one worker node with a single core. Then check the <strong>I agree to the terms and conditions stated above</strong> box and click the <strong>Purchase</strong> button at the bottom of the blade.</p>
<blockquote>
<p>It is very important to specify "azureuser" as the admin user name, because the scripts that you will use to configure the cluster use that user name.</p>
</blockquote>
<p><a href="Images/template-parameters.png" target="_blank"><img src="Images/template-parameters.png" alt="Deploying the cluster" style="max-width:100%;"></a></p>
<p><em>Deploying the cluster</em></p>
</li>
<li>
<p>Click <strong>Resource groups</strong> in the ribbon on the left. Then click the resource group created for the cluster.</p>
<p><a href="Images/open-cluster-resource-group.png" target="_blank"><img src="Images/open-cluster-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Wait until "Deploying" changes to "Succeeded," indicating that the cluster has been successfully deployed. It generally takes about five minutes for the deployment to complete for a cluster with a single worker node, and more for clusters containing more nodes.</p>
<blockquote>
<p>Click the browser's <strong>Refresh</strong> button occasionally to update the deployment status. Clicking the <strong>Refresh</strong> button in the resource-group blade refreshes the list of resources in the resource group, but does not reliably update the deployment status.</p>
</blockquote>
<p><a href="Images/deployment-succeeded.png" target="_blank"><img src="Images/deployment-succeeded.png" alt="Successful deployment" style="max-width:100%;"></a></p>
<p><em>Successful deployment</em></p>
</li>
</ol>
<p>With the cluster deployed, the next step is to connect to the cluster and configure it to run the scripts you prepared in Exercise 1. If you are running macOS or Linux, proceed to <a href="#Exercise3">Exercise 3</a>. If you are running Windows, skip to <a href="#Exercise4">Exercise 4</a>.</p>
<p><a name="Exercise3"></a></p>
<h2>Exercise 3 (macOS and Linux): Connect to and configure the cluster</h2>
<p>In this exercise, you will upload the Python scripts that you modified in Exercise 1 and a pair of shell scripts to the master node of the cluster and use the shell scripts to configure the cluster.</p>
<ol>
<li>
<p>In the list of resources that comprise the cluster in the cluster's resource-group blade, click <strong>publicip</strong>.</p>
<p><a href="Images/open-public-ip.png" target="_blank"><img src="Images/open-public-ip.png" alt="Opening the publicip resource" style="max-width:100%;"></a></p>
<p><em>Opening the publicip resource</em></p>
</li>
<li>
<p>Click the <strong>Copy</strong> button to the right of the DNS name to copy the master node's DNS name to the clipboard.</p>
<p><a href="Images/copy-dns-name.png" target="_blank"><img src="Images/copy-dns-name.png" alt="Copying the DNS name" style="max-width:100%;"></a></p>
<p><em>Copying the DNS name</em></p>
</li>
<li>
<p>Open a terminal window and navigate to the directory containing the Python scripts you modified in <a href="#Exercise1">Exercise 1</a>.</p>
</li>
<li>
<p>Execute the following command in the terminal window, replacing <em>masterDNS</em> with the DNS name on the clipboard. When prompted, enter the admin password for the cluster ("Azure4Research!").</p>
<blockquote>
<p>Because this is the first time you have connected to the master node, you will be prompted with a security warning asking if you want to update the cached key. Since the host is one you created, answer yes.</p>
</blockquote>
 <pre> scp * azureuser@<i>masterDNS</i>:.</pre>
</li>
<li>
<p>The next step is to establish an SSH connection to the master node. To do that, execute the command below in the terminal window, once more replacing <em>masterDNS</em> with the DNS name on the clipboard. When prompted for a password, enter the admin password for the cluster ("Azure4Research!").</p>
 <pre> ssh azureuser@<i>masterDNS</i></pre>
</li>
<li>
<p>To be certain that the script files you uploaded to the cluster contain Linux-style line endings ("/r" rather than "/r/n"), execute the following commands in the terminal window to install and run the dos2unix conversion program:</p>
<pre><code>sudo apt-get install dos2unix
dos2unix *
</code></pre>
</li>
<li>
<p>Now execute the command below in the terminal window to configure the nodes in the cluster. It typically takes a few minutes for each node.</p>
<pre><code>sh setup.sh
</code></pre>
</li>
</ol>
<p>The next task is to run a job on the cluster that you just configured. Since Exercise 4 is for Windows users only, proceed directly to <a href="#Exercise5">Exercise 5</a>.</p>
<p><a name="Exercise4"></a></p>
<h2>Exercise 4 (Windows): Connect to and configure the cluster</h2>
<p>In this exercise, you will upload the Python scripts that you modified in Exercise 1 and a pair of shell scripts to the master node of the cluster and use the shell scripts to configure the cluster. To remote into the cluster, you'll use a popular Windows SSH client named PuTTY. If you haven't already installed PuTTY, <a href="http://www.chiark.greenend.org.uk/%7Esgtatham/putty/download.html">download the MSI</a> and install it now.</p>
<ol>
<li>
<p>In the list of resources that comprise the cluster in the cluster's resource-group blade, click <strong>publicip</strong>.</p>
<p><a href="Images/open-public-ip.png" target="_blank"><img src="Images/open-public-ip.png" alt="Opening the publicip resource" style="max-width:100%;"></a></p>
<p><em>Opening the publicip resource</em></p>
</li>
<li>
<p>Click the <strong>Copy</strong> button to the right of the DNS name to copy the master node's DNS name to the clipboard.</p>
<p><a href="Images/copy-dns-name.png" target="_blank"><img src="Images/copy-dns-name.png" alt="Copying the DNS name" style="max-width:100%;"></a></p>
<p><em>Copying the DNS name</em></p>
</li>
<li>
<p>To copy files to the master node, you will use PuTTY's Secure Copy utility, pscp.exe. Open a Command Prompt window and navigate to the directory containing the Python scripts you modified in <a href="#Exercise1">Exercise 1</a>.</p>
</li>
<li>
<p>Execute the following command, replacing <em>masterDNS</em> with the DNS name on the clipboard. When prompted, enter the admin password for the cluster ("Azure4Research!").</p>
<blockquote>
<p>Because this is the first time you have connected to the master node, you will be prompted with a security warning asking if you want to update the cached key. Since the host is one you created, answer yes.</p>
</blockquote>
 <pre> pscp * azureuser@<i>masterDNS</i>:.</pre>
</li>
<li>
<p>Start PuTTY (putty.exe) and paste the DNS name into the <strong>Host Name (or IP address)</strong> field. Then click the <strong>Open</strong> button to initiate a Secure Shell (SSH) connection.</p>
<p><a href="Images/connect-with-putty.png" target="_blank"><img src="Images/connect-with-putty.png" alt="Connecting with PuTTY" style="max-width:100%;"></a></p>
<p><em>Connecting with PuTTY</em></p>
</li>
<li>
<p>A PuTTY terminal window will appear and you will be prompted to <strong>login as</strong>. Log in with the user name ("azureuser") and password ("Azure4Research!") you entered into the deployment template in <a href="#Exercise2">Exercise 2</a>.</p>
</li>
<li>
<p>To be certain that the script files you uploaded to the cluster contain Linux-style line endings ("/r" rather than "/r/n"), execute the following commands in the PuTTY terminal window to install and run the dos2unix conversion program:</p>
<pre><code>sudo apt-get install dos2unix
dos2unix *
</code></pre>
</li>
<li>
<p>Now execute the command below in the PuTTY terminal window to configure the nodes in the cluster. It typically takes a few minutes for each node.</p>
<pre><code>sh setup.sh
</code></pre>
</li>
</ol>
<p>The next task is to run a job on the cluster that you just configured.</p>
<p><a name="Exercise5"></a></p>
<h2>Exercise 5: Run a job and view the results</h2>
<p>In this exercise, you will run <strong>controller.py</strong> on the cluster's master node. <strong>controller.py</strong> performs the compute-intensive task of computing the distances over a sphere between more than 7,300 airports, yielding more than 53 million distances in total. Rather than do the work on the master node, <strong>controller.py</strong> uses SLURM to delegate calculations to the worker nodes and divides the work into the number of "slices" specified in a command-line parameter. You generally want one "slice" for each core in the cluster.</p>
<ol>
<li>
<p>In the terminal window (macOS and Linux) or the PuTTY terminal window (Windows), execute the following command:</p>
<pre><code>python3 controller.py 1
</code></pre>
</li>
<li>
<p>Return to the Azure Portal. Click <strong>Resource groups</strong> in the ribbon on the left, and then click the "ScalingLabResourceGroup" resource group containing the storage account you created in <a href="#Exercise1">Exercise 1</a>.</p>
<p><a href="Images/open-resource-group.png" target="_blank"><img src="Images/open-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Click the resource group's storage account.</p>
<p><a href="Images/open-storage-account.png" target="_blank"><img src="Images/open-storage-account.png" alt="Opening the storage account" style="max-width:100%;"></a></p>
<p><em>Opening the storage account</em></p>
</li>
<li>
<p>Click <strong>Blobs</strong> to view a list of blob containers in the storage account.</p>
<p><a href="Images/view-blob-containers.png" target="_blank"><img src="Images/view-blob-containers.png" alt="Viewing blob containers" style="max-width:100%;"></a></p>
<p><em>Viewing blob containers</em></p>
</li>
<li>
<p>Click <strong>distances</strong> to open the container named "distances." This container was created by the Python code you ran on the cluster.</p>
<p><a href="Images/view-blobs.png" target="_blank"><img src="Images/view-blobs.png" alt="Opening the blob container" style="max-width:100%;"></a></p>
<p><em>Opening the blob container</em></p>
</li>
<li>
<p>Click <strong>log.txt</strong> to open the blob containing the output from the job you ran on the cluster.</p>
<p><a href="Images/view-blob.png" target="_blank"><img src="Images/view-blob.png" alt="Opening the blob" style="max-width:100%;"></a></p>
<p><em>Opening the blob</em></p>
</li>
<li>
<p>Click <strong>Download</strong> to download the blob and view its contents.</p>
<p><a href="Images/download-blob.png" target="_blank"><img src="Images/download-blob.png" alt="Downloading the blob" style="max-width:100%;"></a></p>
<p><em>Downloading the blob</em></p>
</li>
<li>
<p>The Python script that you ran in Step 1 finishes quickly, but the job itself runs asynchronously and will probably take four minutes or more on a cluster consisting of a single worker node with a single core. When the job is complete, <strong>log.txt</strong> will contain something similar to this:</p>
<pre><code>Starting: 2016-12-16 13:15:45
Starting 0-7377:2016-12-16 13:15:45
Finishing 0-7377:2016-12-16 13:19:55
</code></pre>
<p>The first line indicates when the job was started (when <strong>controller.py</strong> was executed). The succeeding lines indicate when each "slice" of the job was started and completed. In this case, because you passed 1 as a command-line parameter to <strong>controller.py</strong>, there is one Starting/Finishing pair.</p>
<p>Download <strong>log.txt</strong> repeatedly until it contains three lines of output similar to the ones above.</p>
</li>
<li>
<p>Compute the wall-clock time required to complete the job by subtracting the time on the first line from the time on the final line. In the example above, the time required is 4 minutes and 10 seconds.</p>
</li>
</ol>
<p>You now have a baseline for a performance comparison: the time the job required on a single worker node with just one core. <strong>Write down the result so you can easily retrieve it later</strong>. In subsequent runs with larger clusters, <strong>log.txt</strong> will be overwritten with newer results.</p>
<p><a name="Exercise6"></a></p>
<h2>Exercise 6: Delete the cluster</h2>
<p>When virtual machines are running, you are being charged — even if the VMs are idle. Therefore, it is advisable to delete virtual machines when they're not longer needed. In this exercise, you'll delete the cluster by deleting the resource group containing the cluster. Deleting the resource group deletes everything in it and prevents any further charges from being incurred for it.</p>
<ol>
<li>
<p>In the <a href="https://portal.azure.com">Azure Portal</a>, click <strong>Resource groups</strong> in the ribbon on the left. Then click the resource group created for the cluster.</p>
<p><a href="Images/open-cluster-resource-group.png" target="_blank"><img src="Images/open-cluster-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Click the <strong>Delete</strong> button at the top of the blade.</p>
<p><a href="Images/delete-resource-group-1.png" target="_blank"><img src="Images/delete-resource-group-1.png" alt="Deleting the resource group" style="max-width:100%;"></a></p>
<p><em>Deleting the resource group</em></p>
</li>
<li>
<p>For safety, you are required to type in the resource group's name. (Once deleted, a resource group cannot be recovered.) Type the name of the resource group. Then click the <strong>Delete</strong> button to delete all of the resources that comprise the cluster.</p>
</li>
</ol>
<p>After a few minutes, the cluster and all of its resources will be deleted. Billing stops when you click the <strong>Delete</strong> button, so you're not charged for the time required to delete the cluster. Similarly, bulling doesn't start until a cluster is fully and successfully deployed.</p>
<p><a name="Exercise7"></a></p>
<h2>Exercise 7: Test with a cluster containing one worker node with eight cores</h2>
<p>In this exercise, you will deploy a new cluster containing a single worker node with eight cores. Then you will run the same job on it and compare the performance of this cluster to the performance of the cluster containing a single worker node with one core.</p>
<ol>
<li>
<p>Repeat Exercises 2-5, but this time, in Exercise 2, Step 2, set <strong>Vm Size</strong> to <strong>Standard_D4_v2</strong> and <strong>Scale Number</strong> to <strong>1</strong>.</p>
<blockquote>
<p>Feel free to use a different resource-group name if you would like (for example, "ClusterResourceGroup2") in case the previous resource group is still being deleted.</p>
</blockquote>
<p><a href="Images/vm-parameters-1.png" target="_blank"><img src="Images/vm-parameters-1.png" alt="Creating a cluster containing one worker node with eight cores" style="max-width:100%;"></a></p>
<p><em>Creating a cluster containing one worker node with eight cores</em></p>
</li>
<li>
<p>Repeat Exercises 5 and 6, but use the following command to run the job in Exercise 5, Step 1:</p>
<pre><code>python3 controller.py 8
</code></pre>
</li>
<li>
<p>This time, the final <strong>log.txt</strong> file will look something like this since the job was divided into eight parts to better leverage the eight cores available:</p>
<pre><code>Starting 1846-2768:2016-12-16 13:48:03
Starting 3692-4614:2016-12-16 13:48:03
Finishing 3692-4614:2016-12-16 13:48:33
Starting 0-922:2016-12-16 13:48:33
Finishing 1846-2768:2016-12-16 13:48:42
Starting 923-1845:2016-12-16 13:48:42
Finishing 0-922:2016-12-16 13:49:03
Starting 2769-3691:2016-12-16 13:49:03
Finishing 923-1845:2016-12-16 13:49:15
Starting 4615-5537:2016-12-16 13:49:16
Finishing 2769-3691:2016-12-16 13:49:33
Starting 5538-6460:2016-12-16 13:49:34
Finishing 4615-5537:2016-12-16 13:49:51
Starting 6461-7377:2016-12-16 13:49:51
Finishing 5538-6460:2016-12-16 13:50:04
Finishing 6461-7377:2016-12-16 13:50:25
</code></pre>
</li>
</ol>
<p>How long did it take for the job to run this time? How does it compare to the result you received when you ran the job on a single core?</p>
<p><a name="Exercise8"></a></p>
<h2>Exercise 8: Test with a cluster containing eight worker nodes with one core each</h2>
<p>In this exercise, you will deploy a new cluster containing eight worker nodes with one core each. Then you will run the same job on it and compare the performance of this cluster to the performance of the other two clusters. Note that it will take longer to deploy and configure the cluster this time due to the increased number of worker nodes.</p>
<ol>
<li>
<p>Repeat Exercises 2-5, but this time, in Exercise 2, Step 2, set <strong>Vm Size</strong> to <strong>Standard_D1_v2</strong> and <strong>Scale Number</strong> to <strong>8</strong>.</p>
<blockquote>
<p>Feel free to use a different resource-group name if you would like (for example, "ClusterResourceGroup3") in case the previous resource group is still being deleted.</p>
</blockquote>
<p><a href="Images/vm-parameters-2.png" target="_blank"><img src="Images/vm-parameters-2.png" alt="Creating a cluster containing eight worker nodes with one core each" style="max-width:100%;"></a></p>
<p><em>Creating a cluster containing eight worker nodes with one core each</em></p>
</li>
<li>
<p>Repeat Exercises 5 and 6, but use the following command to run the job in Exercise 5, Step 1:</p>
<pre><code>python3 controller.py 8
</code></pre>
</li>
<li>
<p>This time, the final <strong>log.txt</strong> file will look something like this:</p>
<pre><code>Starting: 2016-12-16 15:00:23
Starting 923-1845:2016-12-16 15:00:25
Starting 2769-3691:2016-12-16 15:00:25
Starting 5538-6460:2016-12-16 15:00:25
Starting 0-922:2016-12-16 15:00:25
Starting 4615-5537:2016-12-16 15:00:25
Starting 1846-2768:2016-12-16 15:00:25
Starting 3692-4614:2016-12-16 15:00:25
Starting 6461-7377:2016-12-16 15:00:25
Finishing 5538-6460:2016-12-16 15:00:55
Finishing 0-922:2016-12-16 15:00:56
Finishing 4615-5537:2016-12-16 15:00:57
Finishing 3692-4614:2016-12-16 15:00:57
Finishing 923-1845:2016-12-16 15:00:58
Finishing 2769-3691:2016-12-16 15:00:59
Finishing 6461-7377:2016-12-16 15:01:01
Finishing 1846-2768:2016-12-16 15:01:03
</code></pre>
</li>
</ol>
<p>How long did it take for the job to run this time? How does it compare to the results you received when you ran the job on a single core and on a single node containing eight cores?</p>
<p><a name="Exercise9"></a></p>
<h2>Exercise 9: Delete the storage account</h2>
<p>In this exercise, you will clean up the last of the lab's resources by deleting the resource group containing the storage account that you created in <a href="#Exercise1">Exercise1</a>.</p>
<ol>
<li>
<p>In the <a href="https://portal.azure.com">Azure Portal</a>, click <strong>Resource groups</strong> in the ribbon on the left. Then click the resource group that you created in <a href="#Exercise1">Exercise1</a>.</p>
<p><a href="Images/open-resource-group.png" target="_blank"><img src="Images/open-resource-group.png" alt="Opening the resource group" style="max-width:100%;"></a></p>
<p><em>Opening the resource group</em></p>
</li>
<li>
<p>Click the <strong>Delete</strong> button at the top of the blade.</p>
<p><a href="Images/delete-resource-group-2.png" target="_blank"><img src="Images/delete-resource-group-2.png" alt="Deleting the resource group" style="max-width:100%;"></a></p>
<p><em>Deleting the resource group</em></p>
</li>
<li>
<p>For safety, you are required to type in the resource group's name. (Once deleted, a resource group cannot be recovered.) Type the name of the resource group. Then click the <strong>Delete</strong> button to delete the resource group and the storage account that it contains.</p>
</li>
</ol>
<p>After a few minutes, the resource group and all of its resources will be deleted.</p>
<p><a name="Summary"></a></p>
<h2>Summary</h2>
<p>In this hands-on lab, you compared the performance of three HPC clusters running the same compute-intensive job. It's just one data point, but is indicative of the kinds of differences you can expect, especially when using SLURM to distribute the workload among nodes and cores. If you would like to go further in your experimentation, consider trying different types of Azure VMs such as the recently introduced <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-linux-a8-a9-a10-a11-specs">H-series</a> of machines, which are designed for high-end computational needs such as molecular modeling and computational fluid dynamics and feature InfiniBand networking, as well as <a href="http://gpu.azure.com/">N-series</a> machines, which contain GPUs. The results will undoubtedly vary with these machines, and you would expect better performance given that the D1_v2 and D4_v2 VMs you used in these exercises are relatively modest machines.</p>
<p>For more information on the numerous VM sizes available in Azure, refer to <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-windows-sizes">https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-windows-sizes</a> for Windows VMs and <a href="https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-linux-sizes">https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-linux-sizes</a> for Linux VMs.</p>
<hr>
<p>Copyright 2016 Microsoft Corporation. All rights reserved. Except where otherwise noted, these materials are licensed under the terms of the MIT License. You may use them according to the license as is most appropriate for your project. The terms of this license can be found at <a href="https://opensource.org/licenses/MIT">https://opensource.org/licenses/MIT</a>.</p>
</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
